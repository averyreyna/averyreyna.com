---
title: 'Help Supporters'
publishedAt: '2024-05-11'
summary: 'Summary about the Help Supporters project.'
---

<Hero>
  <div>
    Help Supporters is the second project I worked on with Brian Smith at Columbia University.
    For this project, we explored how technology can bridge the social gap between blind
    people and sighted strangers who want to help them.

    _Consider how often you see a blind person getting help from a stranger in public. It's surprisingly rare, right?_

    Yet platforms like BeMyEyes have 13 times more sighted volunteers than blind users. This reveals a
    fascinating paradox: while many sighted people are eager to help, face-to-face assistance
    rarely happens due to complex social barriers affecting both parties. Our research
    investigates how thoughtfully designed technology could facilitate these interactions.
  </div>
</Hero>

<Line />

<TwoCol>
**Shifting Paradigms**

BLV people have developed sophisticated non-visual techniques for navigating spaces like airports which includes using magnifiers, recognizing helpers by uniforms, and building mental maps through exploration. While these methods are highly effective, there remains a communication gap between BLV individuals and potential sighted helpers.

Modern assistive technology design has evolved from focusing solely on the disabled individual to considering the broader social ecosystem, including sighted people who interact with and support BLV individuals. This shift recognizes that true accessibility emerges from the partnership between BLV people, technology, helpers, and the environment.

While remote assistance platforms like BeMyEyes show there's strong willingness from sighted people to help BLV individuals, they miss the unique benefits of face-to-face interaction. This suggests an opportunity for technology to facilitate and enhance in-person assistance while respecting both parties' autonomy and preferences.
</TwoCol>

<Line />

<TwoCol>
**Project Overview**

We developed and tested four prototypes to facilitate face-to-face assistance between BLV and sighted strangers. Our research focused on two key phases of help: making the _initial connection_ and supporting the _actual collaboration_ between helpers.

With these prototypes, we aimed to answer four crucial questions:

1. How should technology facilitate the initial connection between BLV and sighted individuals?
2. What information should be exchanged during this connection?
3. What types of information should be provided during the actual helping process?
4. How this information should be presented to maximize effectiveness while maintaining social comfort?
</TwoCol>

<Line />

<TwoCol>
**Connection Phase**

The **Person-Finder Glasses** helps BLV users locate potential helpers through audio guidance, enabling them to approach people confidently while maintaining appropriate social distance. The system gives BLV users more agency in seeking assistance by supporting natural walking patterns and socially acceptable interactions.

The **Volunteer Platform** is a mobile app that matches BLV users with nearby sighted volunteers based on specific needs and preferences. Users can specify their requirements and time needed, while volunteers indicate their availability and experience level. This creates a structured framework that reduces social anxiety for both parties, allowing them to connect through the app before meeting in person.
</TwoCol>

<TwoCol>
**Collaboration Phase**

The **Pictorial Display** is a wearable screen on the BLV user's chest that shows fun, friendly Bitmoji images to help guide sighted helpers during their interaction. The display provides realtime feedback about proper assistance techniques while creating a more relaxed atmosphere through its lighthearted visual messages, helping both parties communicate more effectively.

The **Vague Directions Flagger** is a smartphone app that helps sighted volunteers give better directions by identifying when they use vague or visually-dependent language. When the app detects imprecise terms or unclear descriptions, it privately suggests more specific alternatives to the helper, allowing them to improve their communication without disrupting the social interaction.
</TwoCol>

<Line />

<TwoCol>
**System Implementation**

**Person-Finder Glasses**: Runs on Microsoft HoloLens using Unity, with face tracking handled through Microsoft's Holographic API. The spatial audio system uses custom algorithms to process directional data and maintain appropriate social distances. The audio feedback adjusts dynamically based on user movement patterns.

**Volunteer Platform**: Created with React.js and focuses on two core services: a real-time location matching engine and secure user communications. The matching engine processes user profiles and preferences to create connections, while the frontend implements mobile-first responsive design with accessibility features.

**Pictorial Display**: Built with HTML/JavaScript that integrates Bitmoji for visual feedback. It runs on a smartphone and includes remote control capabilities for researchers.

**Vague Directions Flagger**: A React Native app that processes speech input to identify vague directional terms when giving help to BLV users. It utilizes NLP to match phrases against a reference database and provides contextual feedback through a suggestion engine.
</TwoCol>

<Line />

<TwoCol>
**Looking Forward**

Our research shows that assistive technologies for social assistance should create structured environments while preserving user autonomy and dignity. The prototypes' success demonstrates the importance of balancing practical functionality with social acceptability, providing a framework for future developments in facilitating inclusive interactions between BLV and sighted individuals in public spaces.
</TwoCol>